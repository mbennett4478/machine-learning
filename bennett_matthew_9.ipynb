{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression - Stochastic Gradient Descent Linear Regression & OLS Polynomial Regression\n",
    "\n",
    "\n",
    "There are four tasks for this recitation:\n",
    "- Perform Linear Regression using sklearn's SGDRegressor\n",
    "- Perform Polynomial Regression using sklearn's OLS method\n",
    "- Plot degree vs. error to choose optimal degree for a fixed set datasize\n",
    "- Plot learning curves to investigate how the size of training data influeces the model performance for various polynomial models\n",
    "\n",
    "You will use the **Red wine** quality dataset from the following link.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "URL: https://archive.ics.uci.edu/ml/datasets/wine+quality\n",
    "\n",
    "The dataset is related to the **Red** variants of the Portuguese \"Vinho Verde\" wine. It provides the physicochemical (inputs) and sensory (the output) variables are available.\n",
    "\n",
    "The dataset consists of characteristics of white wine (e.g., alcohol content, density, amount of citric acid, pH, etc) with target variable \"quality\" representing rating of wine.\n",
    "\n",
    "\n",
    "Given the characteristics of a new, unlabeled wine, the regression task is to predict its \"quality\".\n",
    "\n",
    "Input variables (based on physicochemical tests):\n",
    "- fixed acidity\n",
    "- volatile acidity\n",
    "- citric acid\n",
    "- residual sugar\n",
    "- chlorides\n",
    "- free sulfur dioxide\n",
    "- total sulfur dioxide\n",
    "- density\n",
    "- pH\n",
    "- sulphates\n",
    "- alcohol\n",
    "\n",
    "Output variable (based on sensory data): \n",
    "- quality \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, learning_curve, GridSearchCV, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, SGDRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "First load the data and explore the feature names, target names, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "#df = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "Perform EDA and determine the following:\n",
    "- Are the input variables collinear?\n",
    "- Do you need to drop any input variable column? \n",
    "\n",
    "Drop the redundant columns if need be. However, if dropping a column doesn't achieve the best accuracy, then don't drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Separate Feature Set (Data Matrix X) and Target (1D Vector y)\n",
    "\n",
    "Create a data matrix (X) that contains all features and a 1D target vector (y) containing the target.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Scale The Features\n",
    "\n",
    "We should ensure that all features have a similar scale. Otherwise optimization algorithms (e.g., Gradient Descent based algorithms) will take much longer time to converge.\n",
    "\n",
    "Also, regularization techniques are sensitive to the scale of data. Thus, you must scale the features before applying regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Train and Test Dataset\n",
    "\n",
    "Use the **\"random_state\"** attribute of the \"train_test_split\" function to ensure the reproducability of your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Sklearn's Stochastic Gradient Descent (SGD)\n",
    "\n",
    "\n",
    "Perform linear regression by using sklearn's SGDRegressor. You should use **regularized** SGDRegressor.\n",
    "\n",
    "First, you will select the best model via hyperparameter tuning. Use sklearn's GridSearchCV.\n",
    "\n",
    "Find the best combination of the following hyperparameters. You may add other parameters in this list if you find it reasonable.\n",
    "\n",
    "- alpha\n",
    "- learning_rate (\"constant\", \"optimal\")\n",
    "- eta0\n",
    "- l1_ratio\n",
    "- max_iter\n",
    "- eta0\n",
    "\n",
    "The GridSearchCV takes an argument to define the scoring metric (performance measure). \n",
    "\n",
    "See the list of possible scoring functions:\n",
    "https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "\n",
    "For regression, you may use \"neg_mean_squared_error\" or \"explained_variance\" scoring function. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning for SGD Regressor\n",
    "\n",
    "You should tune the following hyper-parameters. Below I have suggested some values and range, but you are free to explore. Please note that the tuning will take longer time. So, have patience!\n",
    "\n",
    "- alpha: 0.1, 0.01, 0.001\n",
    "- learning_rate: \"constant\", \"optimal\"\n",
    "- l1_ratio': from 0 to 1\n",
    "- max_iter': try larger iterations from 10000\n",
    "- eta0: 0.01, 0.001\n",
    "\n",
    "#### Report the best score (negative mean squared error) & optimal hyperparameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print(\"Best Score (negative mean squared error): %f\" % )\n",
    "#print(\"Optimal Hyperparameter Values: \", )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select The Best Model for the SGD Regressor\n",
    "\n",
    "Using the optimal hyperparameter values, create the best model.\n",
    "Then, fit the model.\n",
    "\n",
    "\n",
    "Report the following two evaluation metrics.\n",
    "\n",
    "- Mean Squared Error (MSE)\n",
    "- Coefficient of Determination or $R^2$/$r^2$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------- Model Evaluation -----------------------------\n"
     ]
    }
   ],
   "source": [
    "# SGD Regression using the best model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n----------------------------- Model Evaluation -----------------------------\")\n",
    "\n",
    "# Make prediction on the training data\n",
    "\n",
    "\n",
    "\n",
    "#print(\"Mean squared error: %.2f\" % )\n",
    "\n",
    "\n",
    "#print(\"Coefficient of determination r^2 variance score [1 is perfect prediction]: %.2f\" % )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model Performance Using Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Make prediction using the test data\n",
    "\n",
    "\n",
    "#print(\"Mean squared error: %.2f\" % )\n",
    "\n",
    "\n",
    "#print(\"Coefficient of determination r^2 variance score [1 is perfect prediction]: %.2f\" % )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=blue>Polynomial Regression Using the OLS Method</font>\n",
    "\n",
    "\n",
    "\n",
    "## Choose the Optimal Degree (Model Complexity) of the Polynomial Regression\n",
    "\n",
    "Vary the degree of the polynomial (degree 1, 2 & 3) and train the sklearn's Linear Regression model (OLS) using the training data. Then, compute the mean squared error (mse) for the test data using the models with varying degree.\n",
    "\n",
    "Finally, plot the **root mean square error (rmse)** values against the varying degree. From this plot find the optimal degree (that gives the smallest rmse).\n",
    "\n",
    "\n",
    "## <font color=red> Note for Assignment 3:\n",
    "\n",
    "For the assignment you will have to use cross-validation to get an estimate of a model’s generalization performance. \n",
    "\n",
    "You will write a function to plot the training and validation root mean square error (rmse) values of the data matrix X for various polynomial degree starting from 1 up to the value set by max polynomial degree. It takes the data matrix X (usually the training data matrix) and the max polynomial degree; and for each polynomial degree it will augment the data matrix, then use k-fold cross-validation to compute the average mse for both the training and the validation fold. For training the model (using the “fit” method) it will use the model parameters from the function argument. Finally, the function will plot the root-mean-square error (rmse) values for the training and validation folds for each degree of the data matrix starting from 1 up to the max polynomial degree.\n",
    "\n",
    "\n",
    "## Model's Complexity: Overfitting or Underfitting\n",
    "If a model performs well on the training data but generalizes poorly (on the validation data) according to the cross-validation metrics, then your model is overfitting. If it performs poorly on both, then it is underfitting. This is one way to tell when a model is too simple or too complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# RMSE vs. Degree Curve\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine Model Complexity using Learning Curve\n",
    "\n",
    "Determine your polynomial regression model's complexity (whether it's overfitting or underfitting) by creating learning curves by varying the degree of polynomial. \n",
    "\n",
    "Use skleatn's function for plotting the learning curves.\n",
    "\n",
    "\n",
    "You will generate two set of learning curves.\n",
    "- Linear Model\n",
    "- 4th Degree Polynomial Model\n",
    "\n",
    "Your goal is to investigate whether your two models are overfitting or underfitting. Under the \"Observation\" block clearly state your conclusion and justification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curve: Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Learning Curve: Linear Model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation: Linear Model Learning Curve\n",
    "\n",
    "#### Conclusion:\n",
    "\n",
    "#### Justification:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curve: Polynomial Model (4th degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Learning Curve: Polynomial (4th Degree) Model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation: Polynomial Model (4th degree) Learning Curve\n",
    "\n",
    "##### Conclusion:\n",
    "\n",
    "\n",
    "\n",
    "##### Justification:"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
